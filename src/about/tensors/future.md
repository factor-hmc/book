## Future Work

Broadly speaking, future work for this vocabulary can be broken up into two major areas: improving `tensor`s’ performance and usability.

There is still room for optimization with the currently implemented words. Many of the implemented words are both more than an order of magnitude slower than NumPy and do not scale as efficiently, most notably `transpose`. This is an ongoing process, as each word can ideally always get faster. (To read more about the current performance of our vocabulary, see the [evaluation](evaluation.md) section.)  In addition, the performance of a number of `sequence` words can be improved by providing `tensor`-specific implementations that take advantage of the underlying structure of the `tensor`. This includes many of the higher-order functions such as `map`, `2map`, and `reduce`. Finally, the `tensors` vocabulary currently only supports 32-bit floats, and allowing `tensor`s to store different types of numbers, including integers and floats of different sizes would allow for additional performance gains where floating point arithmetic is either not necessary or not necessary at that precision. This would involve changing the underlying implementation of `tensor`s and modifying multiple words.

To improve the usability of the `tensors` vocabulary, there are a number of crucial features implemented with NumPy arrays that the `tensors` vocabulary does not provide. (To read more about the features we do provide as well as how they are implemented, see the [methods and design decisions](methods.md) section.) These include [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html), performing operations over specific axes (see the _axes_ variable [here](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)), [index slicing](https://numpy.org/doc/stable/reference/arrays.indexing.html), and more complicated mathematical operations. The versatility that these operations provide would make using `tensor`s much easier and more frictionless. These would be fairly intensive to implement, and would involve modifying current operations as well as adding new words.

Finally, the current vocabulary does not always do a good job of hiding the underlying implementation from the user. Specifically, this becomes a problem when trying to understand error messages. The addition of better error checking—both more specific errors and checking for errors earlier within operations—would make the vocabulary more intuitive and easier to use. An example of this is with the parsing word `t{`, which could have tensor-specific errors for invalid inputs. This would not be hard to implement, and would consist of adding extra cases to the `>tensor` word.
